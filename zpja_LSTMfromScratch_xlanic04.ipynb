{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZODnS3LfIFDU"
      },
      "outputs": [],
      "source": [
        "# keras.datasets.imdb is broken in TensorFlow 1.13 and 1.14 due to numpy 1.16.3\n",
        "!pip install numpy==1.16.2\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "SRC_DIR = \"/content/drive/My Drive/FIT/ZPJa\"\n",
        "\n",
        "if not os.path.exists(SRC_DIR):\n",
        "  os.makedirs(SRC_DIR)\n",
        "\n",
        "# Supress deprecation warnings\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "# Fetch \"IMDB Movie Review\" data, constraining our reviews to \n",
        "# the 10000 most commonly used words\n",
        "vocab_size = 10000\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Map for readable classnames\n",
        "class_names = [\"Negative\", \"Positive\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download GloVe embeddings (if not already available on Google Drive)"
      ],
      "metadata": {
        "id": "7-U0JfGnX__-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_download = True\n",
        "if (glove_download):\n",
        "  !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "  !unzip glove*.zip"
      ],
      "metadata": {
        "id": "oQ_JRRlqYFW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqlm_QDvARPH"
      },
      "source": [
        "Get IMDB dataset word index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4683yGVKsIu"
      },
      "outputs": [],
      "source": [
        "def print_first_few_elements(dictionary: dict):\n",
        "  i = 0\n",
        "  for k, v in dictionary.items():\n",
        "    print(k,v)\n",
        "    i = i + 1\n",
        "    if (i==10):\n",
        "      print('-' * 10)\n",
        "      return\n",
        "\n",
        "# Get the word index from the dataset\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "\n",
        "# Ensure that \"special\" words are mapped into human readable terms \n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNKNOWN>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftXTeNp8V_LK"
      },
      "source": [
        "Parse the pretrained GloVE 50d-embeddings into a dictionary (unless created in any of the previous runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l7QpK40V-dA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "os.listdir(SRC_DIR)\n",
        "EMBED_DICT_TEMP_FILE = os.path.join(SRC_DIR, 'embedding_dict_temp_400k.tmp')\n",
        "GLOVE_EMBEDDINGS_PATH = os.path.join(SRC_DIR, 'glove.6B.50d.txt')\n",
        "\n",
        "if not (os.path.isfile(EMBED_DICT_TEMP_FILE)):\n",
        "  print(\"Creating temporary embedding dictionary...\")\n",
        "  embed_dict = {}\n",
        "  with open(GLOVE_EMBEDDINGS_PATH, \"r\") as f:\n",
        "      data = f.readlines()\n",
        "      for line in data:\n",
        "          line_tokens = line.split()\n",
        "          embed_dict[line_tokens[0]] = [float(embed_val) for embed_val in line_tokens[1:]]\n",
        "\n",
        "      with open(EMBED_DICT_TEMP_FILE, 'w') as out_f:\n",
        "          out_f.write(json.dumps(embed_dict))\n",
        "\n",
        "with open(EMBED_DICT_TEMP_FILE) as json_file:\n",
        "  embed_dict = json.load(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUnnBFvHCa8g"
      },
      "source": [
        "How many words do our reviews contain? And what do our review look like in machine and human readable form?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBcw_ZbJWmkO"
      },
      "outputs": [],
      "source": [
        "# Map for readable classnames\n",
        "class_names = [\"Negative\", \"Positive\"]\n",
        "\n",
        "# Concatenate test and training datasets\n",
        "allreviews = np.concatenate((x_train, x_test), axis=0)\n",
        "\n",
        "# Review lengths across test and training whole datasets\n",
        "print(\"Maximum review length: {}\".format(len(max((allreviews), key=len))))\n",
        "print(\"Minimum review length: {}\".format(len(min((allreviews), key=len))))\n",
        "result = [len(x) for x in allreviews]\n",
        "print(\"Mean review length: {}\".format(np.mean(result)))\n",
        "\n",
        "# Print a review and it's class as stored in the dataset. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Machine readable Review\")\n",
        "print(\"  Review Text: \" + str(x_train[159]))\n",
        "print(\"  Review Sentiment: \" + str(y_train[159]))\n",
        "\n",
        "# Perform reverse word lookup and make it callable\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# Print a review and it's class in human readable format. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Human Readable Review\")\n",
        "print(\"  Review Text: \" + decode_review(x_train[23555]))\n",
        "print(\"  Review Sentiment: \" + class_names[y_train[23555]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GdLwHExPY3A"
      },
      "source": [
        "We need to make sure that our reviews are of a uniform length. Some reviews will need to be truncated, while others need to be padded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGSzmp21ParJ"
      },
      "outputs": [],
      "source": [
        "# Arbitrarily set length of reviews\n",
        "review_length = 300\n",
        "\n",
        "# Padding / truncated our reviews\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = review_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen = review_length)\n",
        "\n",
        "# Check the size of our datasets. \n",
        "print(\"Shape Training Review Data: \" + str(x_train.shape))\n",
        "print(\"Shape Training Class Data: \" + str(y_train.shape))\n",
        "print(\"Shape Test Review Data: \" + str(x_test.shape))\n",
        "print(\"Shape Test Class Data: \" + str(y_test.shape))\n",
        "\n",
        "# Note padding is added to start of review, not the end\n",
        "print(\"\")\n",
        "print(\"Human Readable Review Text (post padding): \" + decode_review(x_train[60]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-4cKD7tF1Wn"
      },
      "source": [
        "Let's initialize the embeddings for 'unknown' words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOU-5WgnF-R4"
      },
      "outputs": [],
      "source": [
        "UNK_EMBEDDING = '-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011 \\\n",
        "0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802 \\\n",
        "0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912 \\\n",
        "0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227 \\\n",
        "0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107 \\\n",
        "-0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544 \\\n",
        "-0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303 \\\n",
        "-0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408 \\\n",
        "0.00653003  0.01685157'\n",
        "\n",
        "unk_embedding = UNK_EMBEDDING.split(\" \")\n",
        "unk_embedding = list(filter(None, unk_embedding))\n",
        "unk_embedding = [float(em) for em in unk_embedding]\n",
        "\n",
        "pad_embedding = [float(0.0)] * 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoMI5uFq8RH7"
      },
      "source": [
        "Now let's build a matrix of weights (pretrained embeddings) that will be loaded into the PyTorch embedding layer. Its shape will be equal to (vocab_size, 50):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8PzFR8o8ot6"
      },
      "outputs": [],
      "source": [
        "vocab_embedded = np.zeros((len(word_index), 50))\n",
        "words_found = 0\n",
        "\n",
        "for word, index in word_index.items():\n",
        "  if (word == '<UNKNOWN>'):\n",
        "    vocab_embedded[index] = unk_embedding\n",
        "  elif (word == '<START>') or (word == '<PAD>'):\n",
        "    vocab_embedded[index] = pad_embedding\n",
        "  elif (word == '<UNUSED>'):\n",
        "    print('unused')\n",
        "    vocab_embedded[index] = unk_embedding\n",
        "  else:\n",
        "    try:\n",
        "      vocab_embedded[index] = embed_dict[word]\n",
        "    except KeyError:\n",
        "      # print('Could not find word in pretrained embeddings: {}'.format(word))\n",
        "      vocab_embedded[index] = unk_embedding\n",
        "\n",
        "# Limit the vocabulary to only <vocab_size> most frequently occuring words\n",
        "vocab_embedded = vocab_embedded[:vocab_size,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D71zZ07TNvIT"
      },
      "source": [
        "Let's define CustomLSTM class and implement its *forward* method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFEEOjh7Oazh"
      },
      "outputs": [],
      "source": [
        "from torch._C import dtype\n",
        "import torch, math\n",
        "import torch.nn as nn\n",
        "import os,sys,humanize,psutil,GPUtil\n",
        "\n",
        "# def mem_report():\n",
        "#   print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
        "  \n",
        "#   GPUs = GPUtil.getGPUs()\n",
        "#   for i, gpu in enumerate(GPUs):\n",
        "#     print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "\n",
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, size_in: int, size_hidden: int):\n",
        "        super().__init__()\n",
        "        self.size_in = size_in\n",
        "        self.size_hidden = size_hidden\n",
        "\n",
        "        # Forget gate: f_t = sigmoid(U_f * x_t + Vf * H_t-1 + b_f_)\n",
        "        # Weight for input x_t (i.e. one token from sequence)\n",
        "        self.U_f = nn.Parameter(torch.Tensor(size_in, size_hidden))\n",
        "        # Weight for previous cell hidden output\n",
        "        self.V_f = nn.Parameter(torch.Tensor(size_hidden, size_hidden))\n",
        "        # Bias\n",
        "        self.bias_f = nn.Parameter(torch.Tensor(size_hidden))\n",
        "\n",
        "        # Input gate: i_t = sigmoid(U_i * x_t + V_i * h_t-1 + b_i)\n",
        "        self.U_i = nn.Parameter(torch.Tensor(size_in, size_hidden))\n",
        "        self.V_i = nn.Parameter(torch.Tensor(size_hidden, size_hidden))\n",
        "        self.bias_i = nn.Parameter(torch.Tensor(size_hidden))\n",
        "\n",
        "        # Output gate: o_t = sigmoid(U_o * x_t + V_o * h_t-1 + b_o)\n",
        "        self.U_o = nn.Parameter(torch.Tensor(size_in, size_hidden))\n",
        "        self.V_o = nn.Parameter(torch.Tensor(size_hidden, size_hidden))\n",
        "        self.bias_o = nn.Parameter(torch.Tensor(size_hidden))\n",
        "\n",
        "        # Update of the long-term memory: c_t+ = tanh(U_c * x_t + V_c * h_t-1 + b_c)\n",
        "        self.U_c = nn.Parameter(torch.Tensor(size_in, size_hidden))\n",
        "        self.V_c = nn.Parameter(torch.Tensor(size_hidden, size_hidden))\n",
        "        self.bias_c = nn.Parameter(torch.Tensor(size_hidden))\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            else:\n",
        "                nn.init.xavier_normal_(param)\n",
        "  \n",
        "    def forward(self, x, init_states=None):\n",
        "        \"\"\"\n",
        "        Implements forward pass of the LSTM layer\n",
        "        :param x: input sequence\n",
        "        :param init_states: (h_t, c_t) tuple\n",
        "            h_t = o_t * tanh(c_t)\n",
        "            c_t = f_t * c_t + i_t * g_t\n",
        "        :return: hidden sequence and (h_t, c_t) tuple\n",
        "        \"\"\"\n",
        "        batch_size, seq_size, dims = x.size()\n",
        "        hidden_seq = []\n",
        "\n",
        "        # If h_t and c_t are not provided, we need to initialize them with zeros\n",
        "        if not init_states:\n",
        "            (h_t, c_t) = (\n",
        "                torch.zeros(batch_size, self.size_hidden).to(x.device),\n",
        "                torch.zeros(batch_size, self.size_hidden).to(x.device)\n",
        "            )\n",
        "        else:\n",
        "            h_t, c_t = init_states\n",
        "\n",
        "        # Iterate over tokens in the sequence 'x' (for all the reviews and embedding\n",
        "        # dimensions simultaneously)\n",
        "        for t in range(seq_size):\n",
        "          x_t = x[:, t, :]\n",
        "\n",
        "          f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.bias_f)\n",
        "          i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.bias_i)\n",
        "          o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.bias_o)\n",
        "          g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.bias_c)\n",
        "          c_t = f_t * c_t + i_t * g_t\n",
        "          h_t = o_t * torch.tanh(c_t)\n",
        "          \n",
        "          hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        return hidden_seq[:, -1, :], (h_t, c_t)\n",
        "\n",
        "class MyFullyConnected(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out):\n",
        "    super().__init__()\n",
        "\n",
        "    # Initialization of linear layer weights and biases (inspired by weight initialization\n",
        "    # of fully connected layer in Pytorch: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "    k = math.sqrt(1.0 / dim_in)\n",
        "    weight_init = np.random.uniform(low=-k, high=k, size=(dim_out,dim_in))\n",
        "    self.weight = nn.Parameter(torch.tensor(weight_init).float())\n",
        "\n",
        "    bias_init = np.random.uniform(low=-k, high=k, size=dim_out)\n",
        "    self.bias = nn.Parameter(torch.tensor(bias_init).float())\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    @param x represents the last element in the given sequence of shape (batch_size, embed_dim)\n",
        "    @returns y = xw^T + b\n",
        "    \"\"\"\n",
        "    return x @ torch.transpose(self.weight, 0, 1) + self.bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF1AIU6PMjNh"
      },
      "source": [
        "Create our NN model including the **CustomLSTM layer** and the **pretrained 50d GloVe embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OokF1AqhPt3U"
      },
      "outputs": [],
      "source": [
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "print(device)\n",
        "\n",
        "\n",
        "class MyLSTMNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    num_embeddings, embedding_dim = vocab_embedded.shape\n",
        "    print('MyLSTMNet - count of total word embeddings in the embedding layer: ', num_embeddings)\n",
        "    self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "    # Init the embedding layer with the pretrained weights\n",
        "    # (Comment the next line if pretrained embeddings are not preferred)\n",
        "    self.embedding.load_state_dict({'weight': torch.tensor(vocab_embedded, requires_grad=True, dtype=torch.float)})\n",
        "    \n",
        "    print('MyLSTMNet - dimension of the word embeddings: ', embedding_dim)\n",
        "    self.lstm = CustomLSTM(embedding_dim, embedding_dim)\n",
        "\n",
        "    # Readout layer\n",
        "    self.fully_connected = MyFullyConnected(embedding_dim, 2)\n",
        "        \n",
        "  def forward(self, x):\n",
        "      x_embedded = self.embedding(x)\n",
        "      lstm_out, _ = self.lstm(x_embedded)\n",
        "      return self.fully_connected(lstm_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJvL6-F0MyZq"
      },
      "source": [
        "Instantiate the MyLSTMNet model and run training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRKyxcsTM7nB"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "net = MyLSTMNet()\n",
        "net.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set up training params\n",
        "MAX_EPOCHS = 10\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "TEST_BATCH_SIZE = 128\n",
        "\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "print('Count of reviews in the training data: ', x_train_tensor.size()[0])\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "            'shuffle': True}\n",
        "          #  'num_workers': 2}\n",
        "          # 'pin_memory': True}\n",
        "\n",
        "ds_train = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(ds_train, **train_params)\n",
        "\n",
        "\n",
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "            'shuffle': True}\n",
        "          #  'num_workers': 2}\n",
        "\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print('Count of reviews in the testing data: ', x_test_tensor.size()[0])\n",
        "\n",
        "ds_test = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(ds_test, **test_params)\n",
        "\n",
        "print('Number of iterations within one training epoch: {} (batch size {})'.format(len(train_loader), TRAIN_BATCH_SIZE))\n",
        "print('Number of iterations within one training epoch: {} (batch size {})'.format(len(test_loader), TEST_BATCH_SIZE))\n",
        "print()\n",
        "\n",
        "epoch_bar = tqdm(range(MAX_EPOCHS), desc=\"LSTM training\", position=0, total=MAX_EPOCHS)\n",
        "accuracy = 0\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "  batch_bar = tqdm(enumerate(train_loader), desc=\"Epoch: {}\".format(str(epoch)), position=1, total=len(train_loader))\n",
        "  \n",
        "  for i, (reviews, sentiments) in enumerate(train_loader):\n",
        "      reviews = reviews.long().to(device)\n",
        "      y_pred = net(reviews)\n",
        "      sentiments_gpu = sentiments.to(device)\n",
        "      loss = criterion(y_pred, sentiments_gpu)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (i + 1) % 10 == 0:\n",
        "        accuracy = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for  i, (data, labels) in enumerate(test_loader):\n",
        "                predictions = net(data.to(device))\n",
        "                accuracy += (predictions.argmax(dim=1) == labels.to(device)).float().sum().cpu().item()\n",
        "        accuracy /= len(x_train)\n",
        "\n",
        "      batch_bar.set_postfix(loss=loss.cpu().item(), accuracy=\"{:.2f}\".format(accuracy), epoch=epoch)\n",
        "      batch_bar.update()\n",
        "\n",
        "  epoch_bar.set_postfix(loss=loss.cpu().item(), accuracy=\"{:.2f}\".format(accuracy), epoch=epoch)\n",
        "  epoch_bar.update()\n",
        "\n",
        "torch.save(net.state_dict(), os.path.join(SRC_DIR, 'exported_model'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LSTMfromScratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}